# Visual Analytics Assignment 4
## Assignment description
This repository contains my submission of the final portfolio assignment for the Visual Analytics course at Aarhus university, the topic of which is self-assigned.     
My goal for this assignment is to write a program that can classify the types of figures used in scientific publications.     
Dataset can be sourced here:    
https://cvit.iiit.ac.in/usodi/Docfig.php    

## Methods 
I loaded data from [the DocFigure dataset](https://cvit.iiit.ac.in/usodi/Docfig.php) to use as training data. The pre-split data was merged to allow me to create my own training/test data split.     
Images with duplicate filenames were excluded from analysis, as well as other files (GIFs and 8 likely corrupted files of different types) that could not be loaded using the opencv-python module.     
Image data was saved as np.arrays in a separate folder to save time when re-loading the dataset.    
I trained two models on the data – A logistic regression model to serve as a baseline, and a model created using VGG16 (loaded using the tensorflow module), a pretrained convolutional neural network, to extract features from the images. A classification layer was added to the pretrained CNN while slowing down stochastic gradient decent (using an exponential decay rate of 0.9) to avoid overfitting.     

## Repository structure 
in: Folder for input data – contains 4 subfolders:    
-	annotation: Contains the y data of the DocFigure dataset 
-	images: Contains the X data of the DocFigure dataset in image format
-	np_arrays: Generated by the nn_data_processing.py script – contains filtered X data as npy files
-	processed_data: Contains filtered y data as well as several snips of data generated by the  nn_data_processing.py script that are used when loading data when running the model

output: Folder for the output generated by the script – at present it contains 3 files:     
-	lr_report.txt: Generated by running the logistic_regression.py script – a .txt file containing the classification report of the model
-	classification_report.txt: Generated by running the cnn_classifier.py – a .txt file containing the classification report of the model
-	history_img.png:  Generated by running cnn_classifier.py – an image containing two graphs – one that tracks the training and validation loss across epochs, and one that does the same for the training and validation accuracy

src: Folder for python scripts     
-	\_\_init\_\_.py
-	nn_data_processing.py
-	logistic_regression.py
-	cnn_classifier.py

utils: Contains utility functions written by course instructor [Ross Deans Kristensen-McLachlan](https://github.com/rdkm89)    
github_link.txt: link to github repository    
requirements.txt: txt file containing the modules required to run the code    

## Usage 
Modules listed in requirements.txt should be installed before scripts are run.      
The nn_data_processing.py script should be run before the rest.    
    
__nn_data_processing.py__    
To clean and process data, run nn_data_processing.py from the Visual_Analytics_A4 repository folder. The script has two arguments:    
- _-a or --average: Average or target value to serve as a baseline for image height - default is 210_
- _-w or --window: Window to be added and subtracted from the average value - values outside the window will be removed – default is 745_

Example of code running the script from the terminal:    
```
python src/nn_data_processing.py -a 210 -w 745
```
__Logistic_regression.py__    
To train and evaluate a logistic regression model, run logistic_regression.py from the Visual_Analytics_A4 repository folder. The script has two arguments:    
- _-s or -size: Value to convert the image height and width to – default is 210_
- _-sub or -subset: Number of values in each category to include in a subset to the data - max 276 – running the function without this argument means that the model will use the full dataset_

Example of code running the script from the terminal:    
```
python src/logistic_regression.py -s 210 -sub 100
```
__cnn_classifier.py__    
To train and evaluate a CNN model, run cnn_classifier.py from the Visual_Analytics_A4 repository folder. The script has one argument:    
- _-sub or -subset: Number of values in each category to include in a subset to the data - max 276 – running the function without this argument means that the model will use the full dataset_

Example of code running the script from the terminal:    
```
python src/cnn_classifier.py -sub 276
```

## Discussion of results
All scripts were run with the default parameters on the full dataset. The CNN ran for 5 epochs.     
Neither the logistic regression nor the CNN seem to perform very well on the full dataset.     
The logistic regression seems to perform slightly better than the CNN. Though, as most of the precision and recall scores are below 0.50, it is unlikely to be of practical use when one has to identify a large group of figures.     
In contrast, the precision and recall and the CNN stays mostly below 0.00. Looking at history_img.png, the trajectories of validation and training accuracy seem somewhat erratic (i.e. while there is a general upwards trend, the validation and training accuracy rarely seem to share a trajectory). This might indicate that the model could benefit from running for more epochs, so as to stabilize the trajectory (though on the other hand, the falling validation accuracy might indicate otherwise).     
Perhaps both models might benefit from sampling the dataset, so as to ensure that each category of figure is equally represented.        

## Full data citation
K V Jobin, Ajoy Mondal and C V Jawahar. (2019). DocFigure [Data file]. Retrieved from https://cvit.iiit.ac.in/usodi/Docfig.php


